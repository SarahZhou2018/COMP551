{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniProject4_CNNSOFTMAXmnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDdWTGqUtOtc"
      },
      "source": [
        "# Deep Learning using Linear Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXXerQAds2bs"
      },
      "source": [
        "https://researchcode.com/code/1486946312/deep-learning-using-linear-support-vector-machines/\n",
        "\n",
        "https://paperswithcode.com/paper/deep-learning-using-linear-support-vector#code\n",
        "\n",
        "https://arxiv.org/pdf/1306.0239v4.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59GssU1AioW"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNoD4DLsBPzV"
      },
      "source": [
        "## Model : CNN SOFTMAX\n",
        "\n",
        "https://github.com/AFAgarap/cnn-svm/tree/master/model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdzXUQbXBOIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a042623-cbc9-4017-96ac-caff3475e998"
      },
      "source": [
        "# Copyright 2017-2020 Abien Fred Agarap\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"2 Convolutional Layers with Max Pooling CNN\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "__version__ = \"0.1.0\"\n",
        "__author__ = \"Abien Fred Agarap\"\n",
        "\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_v2_behavior() \n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "class CNN:\n",
        "    def __init__(self, alpha, batch_size, num_classes, num_features):\n",
        "        \"\"\"Initializes the CNN-Softmax model\n",
        "        :param alpha: The learning rate to be used by the model.\n",
        "        :param batch_size: The number of batches to use for training/validation/testing.\n",
        "        :param num_classes: The number of classes in the dataset.\n",
        "        :param num_features: The number of features in the dataset.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.name = \"CNN-Softmax\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "\n",
        "        def __graph__():\n",
        "\n",
        "            with tf.compat.v1.name_scope(\"input\"):\n",
        "                # [BATCH_SIZE, NUM_FEATURES]\n",
        "                x_input = tf.compat.v1.placeholder(\n",
        "                    dtype=tf.compat.v1.float32, shape=[None, num_features], name=\"x_input\"\n",
        "                )\n",
        "\n",
        "                # [BATCH_SIZE, NUM_CLASSES]\n",
        "                y_input = tf.compat.v1.placeholder(\n",
        "                    dtype= tf.compat.v1.float32, shape=[None, num_classes], name=\"actual_label\"\n",
        "                )\n",
        "\n",
        "            # First convolutional layer\n",
        "            first_conv_weight = self.weight_variable([5, 5, 1, 32])\n",
        "            print(first_conv_weight)\n",
        "            first_conv_bias = self.bias_variable([32])\n",
        "            print(first_conv_bias)\n",
        "\n",
        "            input_image = tf.compat.v1.reshape(x_input, [-1, 28, 28, 1])\n",
        "\n",
        "            first_conv_activation = tf.compat.v1.nn.relu(\n",
        "                self.conv2d(input_image, first_conv_weight) + first_conv_bias\n",
        "            )\n",
        "            first_conv_pool = self.max_pool_2x2(first_conv_activation)\n",
        "\n",
        "            # Second convolutional layer\n",
        "            second_conv_weight = self.weight_variable([5, 5, 32, 64])\n",
        "            second_conv_bias = self.bias_variable([64])\n",
        "\n",
        "            second_conv_activation = tf.compat.v1.nn.relu(\n",
        "                self.conv2d(first_conv_pool, second_conv_weight) + second_conv_bias\n",
        "            )\n",
        "            second_conv_pool = self.max_pool_2x2(second_conv_activation)\n",
        "\n",
        "            # Fully-connected layer (Dense Layer)\n",
        "            dense_layer_weight = self.weight_variable([7 * 7 * 64, 1024])\n",
        "            dense_layer_bias = self.bias_variable([1024])\n",
        "\n",
        "            second_conv_pool_flatten = tf.compat.v1.reshape(second_conv_pool, [-1, 7 * 7 * 64])\n",
        "            dense_layer_activation = tf.compat.v1.nn.relu(\n",
        "                tf.compat.v1.matmul(second_conv_pool_flatten, dense_layer_weight)\n",
        "                + dense_layer_bias\n",
        "            )\n",
        "\n",
        "            # Dropout, to avoid over-fitting\n",
        "            keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
        "            h_fc1_drop = tf.compat.v1.nn.dropout(dense_layer_activation, keep_prob)\n",
        "\n",
        "            # Readout layer\n",
        "            readout_weight = self.weight_variable([1024, num_classes])\n",
        "            readout_bias = self.bias_variable([num_classes])\n",
        "\n",
        "            output = tf.compat.v1.matmul(h_fc1_drop, readout_weight) + readout_bias\n",
        "\n",
        "            with tf.compat.v1.name_scope(\"softmax\"):\n",
        "                loss = tf.compat.v1.reduce_mean(\n",
        "                    tf.compat.v1.nn.softmax_cross_entropy_with_logits(\n",
        "                        logits=output, labels=y_input\n",
        "                    )\n",
        "                )\n",
        "            tf.compat.v1.summary.scalar(\"loss\", loss)\n",
        "\n",
        "            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
        "\n",
        "            with tf.compat.v1.name_scope(\"accuracy\"):\n",
        "                output = tf.compat.v1.identity(tf.nn.softmax(output), name=\"prediction\")\n",
        "                correct_prediction = tf.compat.v1.equal(\n",
        "                    tf.compat.v1.argmax(output, 1), tf.compat.v1.argmax(y_input, 1)\n",
        "                )\n",
        "                with tf.compat.v1.name_scope(\"accuracy\"):\n",
        "                    accuracy = tf.compat.v1.reduce_mean(tf.compat.v1.cast(correct_prediction, tf.compat.v1.float32))\n",
        "            tf.compat.v1.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "            merged = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "            self.x_input = x_input\n",
        "            self.y_input = y_input\n",
        "            self.keep_prob = keep_prob\n",
        "            self.output = output\n",
        "            self.loss = loss\n",
        "            self.optimizer = optimizer\n",
        "            self.accuracy = accuracy\n",
        "            self.merged = merged\n",
        "\n",
        "        sys.stdout.write(\"\\n<log> Building graph...\")\n",
        "        __graph__()\n",
        "        sys.stdout.write(\"</log>\\n\")\n",
        "\n",
        "    def train(self, checkpoint_path, epochs, log_path, train_data, test_data):\n",
        "        \"\"\"Trains the initialized model.\n",
        "        :param checkpoint_path: The path where to save the trained model.\n",
        "        :param epochs: The number of passes through the entire dataset.\n",
        "        :param log_path: The path where to save the TensorBoard logs.\n",
        "        :param train_data: The training dataset.\n",
        "        :param test_data: The testing dataset.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path=log_path):\n",
        "            os.mkdir(log_path)\n",
        "\n",
        "        if not os.path.exists(path=checkpoint_path):\n",
        "            os.mkdir(checkpoint_path)\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=4)\n",
        "\n",
        "        init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "        timestamp = str(time.asctime())\n",
        "\n",
        "        train_writer = tf.compat.v1.summary.FileWriter(\n",
        "            logdir=log_path + timestamp + \"-training\", graph=tf.get_default_graph()\n",
        "        )\n",
        "\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            checkpoint = tf.compat.v1.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                saver = tf.compat.v1.train.import_meta_graph(\n",
        "                    checkpoint.model_checkpoint_path + \".meta\"\n",
        "                )\n",
        "                saver.restore(sess, tf.compat.v1.train.latest_checkpoint(checkpoint_path))\n",
        "\n",
        "            for index in range(epochs):\n",
        "                # train by batch\n",
        "                batch_features, batch_labels = train_data.next_batch(self.batch_size)\n",
        "\n",
        "                # input dictionary with dropout of 50%\n",
        "                feed_dict = {\n",
        "                    self.x_input: batch_features,\n",
        "                    self.y_input: batch_labels,\n",
        "                    self.keep_prob: 0.5,\n",
        "                }\n",
        "\n",
        "                # run the train op\n",
        "                summary, _, loss = sess.run(\n",
        "                    [self.merged, self.optimizer, self.loss], feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                # every 100th step and at 0,\n",
        "                if index % 100 == 0:\n",
        "                    feed_dict = {\n",
        "                        self.x_input: batch_features,\n",
        "                        self.y_input: batch_labels,\n",
        "                        self.keep_prob: 1.0,\n",
        "                    }\n",
        "\n",
        "                    # get the accuracy of training\n",
        "                    train_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "                    # display the training accuracy\n",
        "                    print(\n",
        "                        \"step: {}, training accuracy : {}, training loss : {}\".format(\n",
        "                            index, train_accuracy, loss\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    train_writer.add_summary(summary=summary, global_step=index)\n",
        "\n",
        "                    saver.save(\n",
        "                        sess,\n",
        "                        save_path=os.path.join(checkpoint_path, self.name),\n",
        "                        global_step=index,\n",
        "                    )\n",
        "\n",
        "            test_features = test_data.images\n",
        "            test_labels = test_data.labels\n",
        "\n",
        "            feed_dict = {\n",
        "                self.x_input: test_features,\n",
        "                self.y_input: test_labels,\n",
        "                self.keep_prob: 1.0,\n",
        "            }\n",
        "\n",
        "            test_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "            print(\"Test Accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_variable(shape):\n",
        "        \"\"\"Returns a weight matrix consisting of arbitrary values.\n",
        "        :param shape: The shape of the weight matrix to create.\n",
        "        :return: The weight matrix consisting of arbitrary values.\n",
        "        \"\"\"\n",
        "        initial = tf.compat.v1.truncated_normal(shape, stddev=0.1)\n",
        "        print(initial)\n",
        "        return tf.compat.v1.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def bias_variable(shape):\n",
        "        \"\"\"Returns a bias matrix consisting of 0.1 values.\n",
        "        :param shape: The shape of the bias matrix to create.\n",
        "        :return: The bias matrix consisting of 0.1 values.\n",
        "        \"\"\"\n",
        "        initial = tf.compat.v1.constant(0.1, shape=shape)\n",
        "        print(initial)\n",
        "        return tf.compat.v1.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(features, weight):\n",
        "        \"\"\"Produces a convolutional layer that filters an image subregion\n",
        "        :param features: The layer input.\n",
        "        :param weight: The size of the layer filter.\n",
        "        :return: Returns a convolutional layer.\n",
        "        \"\"\"\n",
        "        return tf.compat.v1.nn.conv2d(features, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "    @staticmethod\n",
        "    def max_pool_2x2(features):\n",
        "        \"\"\"Downnsamples the image based on convolutional layer\n",
        "        :param features: The input to downsample.\n",
        "        :return: Downsampled input.\n",
        "        \"\"\"\n",
        "        return tf.compat.v1.nn.max_pool(\n",
        "            features, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUMQXKUABXgH"
      },
      "source": [
        "## Model : CNN SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPqUtIkxBWpC"
      },
      "source": [
        "# Copyright 2017-2020 Abien Fred Agarap\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"2 Convolutional Layers with Max Pooling CNN\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "__version__ = \"0.1.0\"\n",
        "__author__ = \"Abien Fred Agarap\"\n",
        "\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_v2_behavior() \n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "class CNNSVM:\n",
        "    def __init__(self, alpha, batch_size, num_classes, num_features, penalty_parameter):\n",
        "        \"\"\"Initializes the CNN-SVM model\n",
        "        :param alpha: The learning rate to be used by the model.\n",
        "        :param batch_size: The number of batches to use for training/validation/testing.\n",
        "        :param num_classes: The number of classes in the dataset.\n",
        "        :param num_features: The number of features in the dataset.\n",
        "        :param penalty_parameter: The SVM C penalty parameter.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.name = \"CNN-SVM\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "        self.penalty_parameter = penalty_parameter\n",
        "\n",
        "        def __graph__():\n",
        "\n",
        "            with tf.name_scope(\"input\"):\n",
        "                # [BATCH_SIZE, NUM_FEATURES]\n",
        "                x_input = tf.placeholder(\n",
        "                    dtype=tf.float32, shape=[None, num_features], name=\"x_input\"\n",
        "                )\n",
        "\n",
        "                # [BATCH_SIZE, NUM_CLASSES]\n",
        "                y_input = tf.placeholder(\n",
        "                    dtype=tf.float32, shape=[None, num_classes], name=\"actual_label\"\n",
        "                )\n",
        "\n",
        "            # First convolutional layer\n",
        "            first_conv_weight = self.weight_variable([5, 5, 1, 32])\n",
        "            first_conv_bias = self.bias_variable([32])\n",
        "\n",
        "            input_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
        "\n",
        "            first_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(input_image, first_conv_weight) + first_conv_bias\n",
        "            )\n",
        "            first_conv_pool = self.max_pool_2x2(first_conv_activation)\n",
        "\n",
        "            # Second convolutional layer\n",
        "            second_conv_weight = self.weight_variable([5, 5, 32, 64])\n",
        "            second_conv_bias = self.bias_variable([64])\n",
        "\n",
        "            second_conv_activation = tf.nn.relu(\n",
        "                self.conv2d(first_conv_pool, second_conv_weight) + second_conv_bias\n",
        "            )\n",
        "            second_conv_pool = self.max_pool_2x2(second_conv_activation)\n",
        "\n",
        "            # Fully-connected layer (Dense Layer)\n",
        "            dense_layer_weight = self.weight_variable([7 * 7 * 64, 1024])\n",
        "            dense_layer_bias = self.bias_variable([1024])\n",
        "\n",
        "            second_conv_pool_flatten = tf.reshape(second_conv_pool, [-1, 7 * 7 * 64])\n",
        "            dense_layer_activation = tf.nn.relu(\n",
        "                tf.matmul(second_conv_pool_flatten, dense_layer_weight)\n",
        "                + dense_layer_bias\n",
        "            )\n",
        "\n",
        "            # Dropout, to avoid over-fitting\n",
        "            keep_prob = tf.placeholder(tf.float32)\n",
        "            h_fc1_drop = tf.nn.dropout(dense_layer_activation, keep_prob)\n",
        "\n",
        "            # Readout layer\n",
        "            readout_weight = self.weight_variable([1024, num_classes])\n",
        "            readout_bias = self.bias_variable([num_classes])\n",
        "\n",
        "            output = tf.matmul(h_fc1_drop, readout_weight) + readout_bias\n",
        "\n",
        "            with tf.name_scope(\"svm\"):\n",
        "                regularization_loss = tf.reduce_mean(tf.square(readout_weight))\n",
        "                hinge_loss = tf.reduce_mean(\n",
        "                    tf.square(\n",
        "                        tf.maximum(\n",
        "                            tf.zeros([batch_size, num_classes]), 1 - y_input * output\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                with tf.name_scope(\"loss\"):\n",
        "                    loss = regularization_loss + penalty_parameter * hinge_loss\n",
        "            tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
        "\n",
        "            with tf.name_scope(\"accuracy\"):\n",
        "                output = tf.identity(tf.sign(output), name=\"prediction\")\n",
        "                correct_prediction = tf.equal(\n",
        "                    tf.compat.v1.argmax(output, 1), tf.compat.v1.argmax(y_input, 1)\n",
        "                )\n",
        "                with tf.compat.v1.name_scope(\"accuracy\"):\n",
        "                    accuracy = tf.compat.v1.reduce_mean(tf.compat.v1.cast(correct_prediction, tf.compat.v1.float32))\n",
        "            tf.compat.v1.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "            merged = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "            self.x_input = x_input\n",
        "            self.y_input = y_input\n",
        "            self.keep_prob = keep_prob\n",
        "            self.output = output\n",
        "            self.loss = loss\n",
        "            self.optimizer = optimizer\n",
        "            self.accuracy = accuracy\n",
        "            self.merged = merged\n",
        "\n",
        "        sys.stdout.write(\"\\n<log> Building graph...\")\n",
        "        __graph__()\n",
        "        sys.stdout.write(\"</log>\\n\")\n",
        "\n",
        "    def train(self, checkpoint_path, epochs, log_path, train_data, test_data):\n",
        "        \"\"\"Trains the initialized model.\n",
        "        :param checkpoint_path: The path where to save the trained model.\n",
        "        :param epochs: The number of passes through the entire dataset.\n",
        "        :param log_path: The path where to save the TensorBoard logs.\n",
        "        :param train_data: The training dataset.\n",
        "        :param test_data: The testing dataset.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path=log_path):\n",
        "            os.mkdir(log_path)\n",
        "\n",
        "        if not os.path.exists(path=checkpoint_path):\n",
        "            os.mkdir(checkpoint_path)\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=4)\n",
        "\n",
        "        init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "        timestamp = str(time.asctime())\n",
        "\n",
        "        train_writer = tf.compat.v1.summary.FileWriter(\n",
        "            logdir=log_path + timestamp + \"-training\", graph=tf.compat.v1.get_default_graph()\n",
        "        )\n",
        "\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            checkpoint = tf.compat.v1.train.get_checkpoint_state(checkpoint_path)\n",
        "\n",
        "            if checkpoint and checkpoint.model_checkpoint_path:\n",
        "                saver = tf.compat.v1.train.import_meta_graph(\n",
        "                    checkpoint.model_checkpoint_path + \".meta\"\n",
        "                )\n",
        "                saver.restore(sess, tf.compat.v1.train.latest_checkpoint(checkpoint_path))\n",
        "\n",
        "            for index in range(epochs):\n",
        "                # train by batch\n",
        "                batch_features, batch_labels = train_data.next_batch(self.batch_size)\n",
        "                batch_labels[batch_labels == 0] = -1\n",
        "\n",
        "                # input dictionary with dropout of 50%\n",
        "                feed_dict = {\n",
        "                    self.x_input: batch_features,\n",
        "                    self.y_input: batch_labels,\n",
        "                    self.keep_prob: 0.5,\n",
        "                }\n",
        "\n",
        "                # run the train op\n",
        "                summary, _, loss = sess.run(\n",
        "                    [self.merged, self.optimizer, self.loss], feed_dict=feed_dict\n",
        "                )\n",
        "\n",
        "                # every 100th step and at 0,\n",
        "                if index % 100 == 0:\n",
        "                    feed_dict = {\n",
        "                        self.x_input: batch_features,\n",
        "                        self.y_input: batch_labels,\n",
        "                        self.keep_prob: 1.0,\n",
        "                    }\n",
        "\n",
        "                    # get the accuracy of training\n",
        "                    train_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "                    # display the training accuracy\n",
        "                    print(\n",
        "                        \"step: {}, training accuracy : {}, training loss : {}\".format(\n",
        "                            index, train_accuracy, loss\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    train_writer.add_summary(summary=summary, global_step=index)\n",
        "\n",
        "                    saver.save(\n",
        "                        sess,\n",
        "                        save_path=os.path.join(checkpoint_path, self.name),\n",
        "                        global_step=index,\n",
        "                    )\n",
        "\n",
        "            test_features = test_data.images\n",
        "            test_labels = test_data.labels\n",
        "            test_labels[test_labels == 0] = -1\n",
        "\n",
        "            feed_dict = {\n",
        "                self.x_input: test_features,\n",
        "                self.y_input: test_labels,\n",
        "                self.keep_prob: 1.0,\n",
        "            }\n",
        "\n",
        "            test_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "\n",
        "            print(\"Test Accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_variable(shape):\n",
        "        \"\"\"Returns a weight matrix consisting of arbitrary values.\n",
        "        :param shape: The shape of the weight matrix to create.\n",
        "        :return: The weight matrix consisting of arbitrary values.\n",
        "        \"\"\"\n",
        "        initial = tf.compat.v1.truncated_normal(shape, stddev=0.1)\n",
        "        return tf.compat.v1.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def bias_variable(shape):\n",
        "        \"\"\"Returns a bias matrix consisting of 0.1 values.\n",
        "        :param shape: The shape of the bias matrix to create.\n",
        "        :return: The bias matrix consisting of 0.1 values.\n",
        "        \"\"\"\n",
        "        initial = tf.compat.v1.constant(0.1, shape=shape)\n",
        "        return tf.compat.v1.Variable(initial)\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(features, weight):\n",
        "        \"\"\"Produces a convolutional layer that filters an image subregion\n",
        "        :param features: The layer input.\n",
        "        :param weight: The size of the layer filter.\n",
        "        :return: Returns a convolutional layer.\n",
        "        \"\"\"\n",
        "        return tf.compat.v1.nn.conv2d(features, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "\n",
        "    @staticmethod\n",
        "    def max_pool_2x2(features):\n",
        "        \"\"\"Downnsamples the image based on convolutional layer\n",
        "        :param features: The input to downsample.\n",
        "        :return: Downsampled input.\n",
        "        \"\"\"\n",
        "        return tf.compat.v1.nn.max_pool(\n",
        "            features, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rxD-QfmBw8b"
      },
      "source": [
        "## MNIST Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgtyHjF8Elh6",
        "outputId": "11396b05-46d0-47b4-bd00-f2897285586c"
      },
      "source": [
        "! git clone https://github.com/uroosehar1/tensorflow-uroo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-uroo'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_cDjLhAFUrO",
        "outputId": "cf34d862-2005-4d74-ca6b-6c576e875475"
      },
      "source": [
        "!unzip './tensorflow-uroo/mnist.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./tensorflow-uroo/mnist.zip\n",
            "   creating: mnist/\n",
            "  inflating: mnist/mnist_with_summaries.py  \n",
            "  inflating: mnist/.DS_Store         \n",
            "  inflating: __MACOSX/mnist/._.DS_Store  \n",
            "  inflating: mnist/__init__.py       \n",
            "  inflating: mnist/mnist_softmax_xla.py  \n",
            "  inflating: mnist/fully_connected_feed.py  \n",
            "  inflating: mnist/BUILD             \n",
            "  inflating: mnist/input_data.py     \n",
            "  inflating: mnist/mnist.py          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37QNj9jGc6A"
      },
      "source": [
        "!mv ./mnist/input_data.py input_data.py\n",
        "import input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1yf25YS1KLp",
        "outputId": "4fb77edf-7015-4d3c-9021-58811f3e8777"
      },
      "source": [
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from tf.compat.v1.examples.tutorials.mnist import input_data\n",
        "# from tensorflow.examples.tutorials.mnist.input_data import input_data\n",
        "# import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
        "# mnist = tf.compat.v1.datasets.load('mnist')\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "num_classes = mnist.train.labels.shape[1]\n",
        "sequence_length = mnist.train.images.shape[1]\n",
        "# (x_train, y_train), (x_test, y_test) = tf.compat.v1.keras.datasets.mnist.load_data()\n",
        "# x_train.labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-25b84d57eb31>:8: read_data_sets (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
            "WARNING:tensorflow:From /content/input_data.py:297: _maybe_download (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /content/input_data.py:299: _extract_images (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /content/input_data.py:304: _extract_labels (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /content/input_data.py:112: _dense_to_one_hot (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /content/input_data.py:328: _DataSet.__init__ (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYs9F0T_Qxf1"
      },
      "source": [
        "## Fit Models on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2UqsgRzQ5qv",
        "outputId": "7ea1098e-284d-42ac-a97b-05ee6be68466"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_v2_behavior() \n",
        "\n",
        "model = CNN(\n",
        "            alpha=1e-3,\n",
        "            batch_size=200,\n",
        "            num_classes=num_classes,\n",
        "            num_features=sequence_length,\n",
        "        )\n",
        "\n",
        "model.train(\n",
        "            checkpoint_path='./',\n",
        "            epochs=1000,\n",
        "            log_path='./',\n",
        "            train_data=mnist.train,\n",
        "            test_data=mnist.test,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<log> Building graph...Tensor(\"truncated_normal:0\", shape=(5, 5, 1, 32), dtype=float32)\n",
            "<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
            "Tensor(\"Const:0\", shape=(32,), dtype=float32)\n",
            "<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n",
            "Tensor(\"truncated_normal_1:0\", shape=(5, 5, 32, 64), dtype=float32)\n",
            "Tensor(\"Const_1:0\", shape=(64,), dtype=float32)\n",
            "Tensor(\"truncated_normal_2:0\", shape=(3136, 1024), dtype=float32)\n",
            "Tensor(\"Const_2:0\", shape=(1024,), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"truncated_normal_3:0\", shape=(1024, 10), dtype=float32)\n",
            "Tensor(\"Const_3:0\", shape=(10,), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "</log>\n",
            "step: 0, training accuracy : 0.14000000059604645, training loss : 9.868700981140137\n",
            "step: 100, training accuracy : 0.9549999833106995, training loss : 0.21662110090255737\n",
            "step: 200, training accuracy : 0.9850000143051147, training loss : 0.07940499484539032\n",
            "step: 300, training accuracy : 0.9700000286102295, training loss : 0.10406967252492905\n",
            "step: 400, training accuracy : 0.9900000095367432, training loss : 0.0692271739244461\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:970: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step: 500, training accuracy : 0.9850000143051147, training loss : 0.08571243286132812\n",
            "step: 600, training accuracy : 0.9950000047683716, training loss : 0.042037349194288254\n",
            "step: 700, training accuracy : 0.9850000143051147, training loss : 0.05533752590417862\n",
            "step: 800, training accuracy : 1.0, training loss : 0.034523844718933105\n",
            "step: 900, training accuracy : 1.0, training loss : 0.02827920764684677\n",
            "Test Accuracy: 0.9890000224113464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MBWApXVHXPq",
        "outputId": "b25397e4-bd68-4fbe-ba2d-f299cbd56b76"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_v2_behavior() \n",
        "\n",
        "model = CNN(\n",
        "            alpha=1e-3,\n",
        "            batch_size=128,\n",
        "            num_classes=num_classes,\n",
        "            num_features=sequence_length,\n",
        "        )\n",
        "\n",
        "model.train(\n",
        "            checkpoint_path='./',\n",
        "            epochs=10000,\n",
        "            log_path='./',\n",
        "            train_data=mnist.train,\n",
        "            test_data=mnist.test,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<log> Building graph...Tensor(\"truncated_normal:0\", shape=(5, 5, 1, 32), dtype=float32)\n",
            "<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
            "Tensor(\"Const:0\", shape=(32,), dtype=float32)\n",
            "<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n",
            "Tensor(\"truncated_normal_1:0\", shape=(5, 5, 32, 64), dtype=float32)\n",
            "Tensor(\"Const_1:0\", shape=(64,), dtype=float32)\n",
            "Tensor(\"truncated_normal_2:0\", shape=(3136, 1024), dtype=float32)\n",
            "Tensor(\"Const_2:0\", shape=(1024,), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"truncated_normal_3:0\", shape=(1024, 10), dtype=float32)\n",
            "Tensor(\"Const_3:0\", shape=(10,), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "</log>\n",
            "step: 0, training accuracy : 0.15625, training loss : 8.70656967163086\n",
            "step: 100, training accuracy : 0.9375, training loss : 0.23542875051498413\n",
            "step: 200, training accuracy : 0.984375, training loss : 0.1954995095729828\n",
            "step: 300, training accuracy : 0.984375, training loss : 0.12346269190311432\n",
            "step: 400, training accuracy : 0.984375, training loss : 0.12738843262195587\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:970: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step: 500, training accuracy : 0.9921875, training loss : 0.03320709243416786\n",
            "step: 600, training accuracy : 0.9765625, training loss : 0.11333438754081726\n",
            "step: 700, training accuracy : 0.9921875, training loss : 0.08872722089290619\n",
            "step: 800, training accuracy : 0.96875, training loss : 0.16142787039279938\n",
            "step: 900, training accuracy : 1.0, training loss : 0.028303686529397964\n",
            "step: 1000, training accuracy : 0.9921875, training loss : 0.018396321684122086\n",
            "step: 1100, training accuracy : 1.0, training loss : 0.028588708490133286\n",
            "step: 1200, training accuracy : 0.9921875, training loss : 0.069188192486763\n",
            "step: 1300, training accuracy : 1.0, training loss : 0.015597431920468807\n",
            "step: 1400, training accuracy : 0.9921875, training loss : 0.033498331904411316\n",
            "step: 1500, training accuracy : 1.0, training loss : 0.02254408597946167\n",
            "step: 1600, training accuracy : 1.0, training loss : 0.015994736924767494\n",
            "step: 1700, training accuracy : 0.9921875, training loss : 0.0501125194132328\n",
            "step: 1800, training accuracy : 1.0, training loss : 0.03872556984424591\n",
            "step: 1900, training accuracy : 1.0, training loss : 0.04100411385297775\n",
            "step: 2000, training accuracy : 1.0, training loss : 0.043189842253923416\n",
            "step: 2100, training accuracy : 1.0, training loss : 0.04376126080751419\n",
            "step: 2200, training accuracy : 0.9921875, training loss : 0.05961424484848976\n",
            "step: 2300, training accuracy : 1.0, training loss : 0.03436948359012604\n",
            "step: 2400, training accuracy : 1.0, training loss : 0.03297780826687813\n",
            "step: 2500, training accuracy : 0.9921875, training loss : 0.021550491452217102\n",
            "step: 2600, training accuracy : 1.0, training loss : 0.019812434911727905\n",
            "step: 2700, training accuracy : 1.0, training loss : 0.0029712014365941286\n",
            "step: 2800, training accuracy : 1.0, training loss : 0.03166605159640312\n",
            "step: 2900, training accuracy : 1.0, training loss : 0.001072127721272409\n",
            "step: 3000, training accuracy : 1.0, training loss : 0.044648732990026474\n",
            "step: 3100, training accuracy : 1.0, training loss : 0.02692987211048603\n",
            "step: 3200, training accuracy : 1.0, training loss : 0.0021999659948050976\n",
            "step: 3300, training accuracy : 1.0, training loss : 0.048650287091732025\n",
            "step: 3400, training accuracy : 1.0, training loss : 0.012269563972949982\n",
            "step: 3500, training accuracy : 1.0, training loss : 0.0012373870704323053\n",
            "step: 3600, training accuracy : 1.0, training loss : 0.012699893675744534\n",
            "step: 3700, training accuracy : 1.0, training loss : 0.0003603512013796717\n",
            "step: 3800, training accuracy : 1.0, training loss : 0.03876139968633652\n",
            "step: 3900, training accuracy : 1.0, training loss : 0.012213952839374542\n",
            "step: 4000, training accuracy : 1.0, training loss : 0.030279377475380898\n",
            "step: 4100, training accuracy : 1.0, training loss : 0.015079346485435963\n",
            "step: 4200, training accuracy : 1.0, training loss : 0.018290579319000244\n",
            "step: 4300, training accuracy : 1.0, training loss : 0.0034151081927120686\n",
            "step: 4400, training accuracy : 1.0, training loss : 0.013162380084395409\n",
            "step: 4500, training accuracy : 1.0, training loss : 0.019129633903503418\n",
            "step: 4600, training accuracy : 1.0, training loss : 0.011543083935976028\n",
            "step: 4700, training accuracy : 1.0, training loss : 0.0011397376656532288\n",
            "step: 4800, training accuracy : 0.9921875, training loss : 0.0200209878385067\n",
            "step: 4900, training accuracy : 1.0, training loss : 0.01100031565874815\n",
            "step: 5000, training accuracy : 1.0, training loss : 0.005706337280571461\n",
            "step: 5100, training accuracy : 1.0, training loss : 0.04016551747918129\n",
            "step: 5200, training accuracy : 1.0, training loss : 0.022319676354527473\n",
            "step: 5300, training accuracy : 0.9921875, training loss : 0.002547396346926689\n",
            "step: 5400, training accuracy : 1.0, training loss : 0.00011511335469549522\n",
            "step: 5500, training accuracy : 1.0, training loss : 0.004044600296765566\n",
            "step: 5600, training accuracy : 1.0, training loss : 0.00012524188787210733\n",
            "step: 5700, training accuracy : 0.984375, training loss : 0.05318036302924156\n",
            "step: 5800, training accuracy : 1.0, training loss : 0.00021708704298362136\n",
            "step: 5900, training accuracy : 1.0, training loss : 0.00034033082192763686\n",
            "step: 6000, training accuracy : 1.0, training loss : 0.01916133053600788\n",
            "step: 6100, training accuracy : 1.0, training loss : 0.01500607281923294\n",
            "step: 6200, training accuracy : 1.0, training loss : 0.0030223410576581955\n",
            "step: 6300, training accuracy : 1.0, training loss : 0.029882773756980896\n",
            "step: 6400, training accuracy : 0.9921875, training loss : 0.08236809074878693\n",
            "step: 6500, training accuracy : 1.0, training loss : 0.00016163771215360612\n",
            "step: 6600, training accuracy : 1.0, training loss : 0.0004972638562321663\n",
            "step: 6700, training accuracy : 1.0, training loss : 0.00022798165446147323\n",
            "step: 6800, training accuracy : 1.0, training loss : 0.01892608031630516\n",
            "step: 6900, training accuracy : 1.0, training loss : 0.0005777738406322896\n",
            "step: 7000, training accuracy : 1.0, training loss : 0.0010122159728780389\n",
            "step: 7100, training accuracy : 1.0, training loss : 0.0031455981079488993\n",
            "step: 7200, training accuracy : 1.0, training loss : 0.00785738229751587\n",
            "step: 7300, training accuracy : 1.0, training loss : 0.00036133622052147985\n",
            "step: 7400, training accuracy : 1.0, training loss : 9.858342309598811e-06\n",
            "step: 7500, training accuracy : 1.0, training loss : 0.012674912810325623\n",
            "step: 7600, training accuracy : 1.0, training loss : 0.0021897703409194946\n",
            "step: 7700, training accuracy : 1.0, training loss : 0.0009456512634642422\n",
            "step: 7800, training accuracy : 1.0, training loss : 6.729633605573326e-05\n",
            "step: 7900, training accuracy : 1.0, training loss : 0.008985698223114014\n",
            "step: 8000, training accuracy : 1.0, training loss : 0.005524829030036926\n",
            "step: 8100, training accuracy : 1.0, training loss : 0.0010821862379088998\n",
            "step: 8200, training accuracy : 1.0, training loss : 0.01287543773651123\n",
            "step: 8300, training accuracy : 1.0, training loss : 0.0032967650331556797\n",
            "step: 8400, training accuracy : 1.0, training loss : 0.0007473516743630171\n",
            "step: 8500, training accuracy : 1.0, training loss : 0.002015632577240467\n",
            "step: 8600, training accuracy : 1.0, training loss : 0.003560899291187525\n",
            "step: 8700, training accuracy : 0.9921875, training loss : 0.001566399703733623\n",
            "step: 8800, training accuracy : 1.0, training loss : 3.1692881748313084e-05\n",
            "step: 8900, training accuracy : 1.0, training loss : 0.007232351694256067\n",
            "step: 9000, training accuracy : 1.0, training loss : 0.002880447544157505\n",
            "step: 9100, training accuracy : 1.0, training loss : 0.001101899310015142\n",
            "step: 9200, training accuracy : 1.0, training loss : 3.390099300304428e-05\n",
            "step: 9300, training accuracy : 1.0, training loss : 8.446612628176808e-05\n",
            "step: 9400, training accuracy : 1.0, training loss : 1.541773963253945e-05\n",
            "step: 9500, training accuracy : 1.0, training loss : 0.00662298733368516\n",
            "step: 9600, training accuracy : 1.0, training loss : 0.00013681178097613156\n",
            "step: 9700, training accuracy : 1.0, training loss : 0.007154890336096287\n",
            "step: 9800, training accuracy : 1.0, training loss : 0.0011929182801395655\n",
            "step: 9900, training accuracy : 1.0, training loss : 6.242115887289401e-06\n",
            "Test Accuracy: 0.9926999807357788\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}